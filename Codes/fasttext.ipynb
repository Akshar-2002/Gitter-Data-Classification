{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c4b17b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65c4b17b",
        "outputId": "ad034dc5-10b0-4572-f6be-29e3acdfe493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "446f148c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "446f148c",
        "outputId": "32547bf2-f441-4db9-e36b-4cbe04d8c1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import gensim \n",
        "from gensim.models.fasttext import FastText\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import csv\n",
        "import warnings\n",
        "warnings.filterwarnings ('ignore')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2818af5",
      "metadata": {
        "id": "e2818af5"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('GitterCom.csv', header=0, encoding='UTF8')\n",
        "#df['message'].to_csv('alice.txt', sep=\"\\n\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtk3SP9LjxeU",
        "outputId": "aeb87fc1-9add-4b80-9557-244348df9c8b"
      },
      "id": "Rtk3SP9LjxeU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a42ef2d",
      "metadata": {
        "id": "1a42ef2d"
      },
      "outputs": [],
      "source": [
        "sample = open(\"/content/gdrive/My Drive/HPC_GD/alice.txt\", \"r\",encoding=\"utf8\") \n",
        "s = sample.read() \n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = [] \n",
        "for i in sent_tokenize(f): \n",
        "\ttemp = [] \n",
        "\t\n",
        "\t# tokenize the sentence into words \n",
        "\tfor j in word_tokenize(i): \n",
        "\t\ttemp.append(j.lower()) \n",
        "\n",
        "\tdata.append(temp)\n",
        "    \n",
        "model = FastText(data, min_count = 1, size = 100, window = 5, sg = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41061b7f",
      "metadata": {
        "id": "41061b7f"
      },
      "outputs": [],
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS =nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def clean_text(text):\n",
        "    if type(text)==str:\n",
        "        \n",
        "        text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "        text = text.lower() # lowercase text\n",
        "        text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "        text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "        text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "        return text\n",
        "    return \"\"\n",
        "def wordprint(words,model1):\n",
        "    all_words, mean = set(), []\n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in model1.wv.vocab:\n",
        "            a=np.array(model1[word])\n",
        "            mean.append(a)\n",
        "    if not mean:\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(100,)   \n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171a77c7",
      "metadata": {
        "id": "171a77c7"
      },
      "outputs": [],
      "source": [
        "def  word_averaging_listn(wv, text_list):\n",
        "    return np.vstack([wordprint(post,wv) for post in text_list ])\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7af8cc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7af8cc2",
        "outputId": "42f76336-9000-45e4-9b12-1edc3cca495c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9963"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "fname='/content/gdrive/My Drive/HPC_GD/GitterCom'+'.csv'\n",
        "df = pd.read_csv(fname,encoding='UTF8')\n",
        "\n",
        "non_string = []\n",
        "for i, row in df.iterrows():\n",
        "    res = ((type(row['message']) != str) or (type(row['Purpose']) != str) or (type(row['Category']) != str) or (type(row['Subcategory']) != str))\n",
        "    if res:\n",
        "        non_string.append(i)\n",
        "\n",
        "df.drop(non_string, axis=0, inplace=True)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2e2417",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a2e2417",
        "outputId": "b68bfcba-bc49-4edf-8bb1-ef7851e82d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             message    Purpose  \\\n",
            "0  hi team recently upgraded cucumberjvm version ...  team wide   \n",
            "1  exception thread main javalangnosuchmethoderro...  team wide   \n",
            "2  amit007 looks like inconsistent cucumber jar v...  team wide   \n",
            "3                        github trying replace irc p  team wide   \n",
            "4  aslakhellesoy thanks seems like using older ve...  team wide   \n",
            "5  hi one question friends starting project studi...  team wide   \n",
            "6  danon9111 integrate cucumber another testing f...  team wide   \n",
            "7  aslakhellesoy reading cucumber book found code...  team wide   \n",
            "8                 sidkiyassine call methods directly  team wide   \n",
            "9  intergrated cucumber+appium https githubcom pr...  team wide   \n",
            "\n",
            "        Category                          Subcategory  \n",
            "0        dev-ops  development operation notifications  \n",
            "1        dev-ops  development operation notifications  \n",
            "2  communication         communication with teammates  \n",
            "3  communication         communication with teammates  \n",
            "4  communication         communication with teammates  \n",
            "5        dev-ops                             team q&a  \n",
            "6        dev-ops                             team q&a  \n",
            "7        dev-ops                             team q&a  \n",
            "8        dev-ops                             team q&a  \n",
            "9        dev-ops  development operation notifications  \n"
          ]
        }
      ],
      "source": [
        "df.drop(['Channel','messageId','time','user'],axis=1,inplace=True)\n",
        "df['Category']=df['Category'].str.lower()\n",
        "df['Purpose']=df['Purpose'].str.lower()\n",
        "df['Subcategory']=df['Subcategory'].str.lower()\n",
        "df['message'] = df['message'].apply(clean_text)\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68578e6b",
      "metadata": {
        "id": "68578e6b"
      },
      "outputs": [],
      "source": [
        "comtdata=df\n",
        "test_tokenized = comtdata.apply(lambda r: w2v_tokenize_text(r['message']), axis=1).values\n",
        "X_comtdata_average1 = word_averaging_listn(model, test_tokenized)\n",
        "fname='7'+'.csv'   #fasttext\n",
        "np.savetxt(fname,X_comtdata_average1, delimiter=',', fmt='%f')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}